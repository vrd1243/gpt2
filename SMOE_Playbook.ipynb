{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z3_FcFzSEoID"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Expert(nn.Module):\n",
        "\n",
        "    def __init__(self, dim):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp1 = nn.Linear(dim, 4*dim)\n",
        "        self.mlp2 = nn.Linear(4*dim, 4*dim)\n",
        "        self.mlp3 = nn.Linear(4*dim, dim)\n",
        "        self.act = nn.LeakyReLU(inplace = True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.act(self.mlp1(x))\n",
        "        x = self.act(self.mlp2(x))\n",
        "        x = self.mlp3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MoE(nn.Module):\n",
        "\n",
        "    def __init__(self, num_experts, dim, max_experts):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_experts = num_experts\n",
        "        self.dim = dim\n",
        "        self.max_experts = max_experts\n",
        "\n",
        "        # We need a router and a set of experts.\n",
        "        self.experts = nn.ModuleList([Expert(dim) for _ in range(num_experts)])\n",
        "        self.router = nn.Linear(dim, num_experts)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Get the gating weights for this input.\n",
        "        # (batch_size, seq_len, num_experts)\n",
        "        expert_weights = self.softmax(self.router(x))\n",
        "\n",
        "        # Obtain the topk weights and their indices.\n",
        "        # (batch_size, seq_len, max_experts)\n",
        "        top_weights, top_indices = torch.topk(expert_weights, self.max_experts, dim=-1, sorted=False)\n",
        "\n",
        "        # Normalize when we scale.\n",
        "        # (batch_size, seq_len, max_experts)\n",
        "        top_weights = top_weights / top_weights.sum(dim=-1, keepdim=True)\n",
        "\n",
        "        for i in range(self.max_experts):\n",
        "\n",
        "            # Get the ith topmost index and the weights.\n",
        "            # (batch_size, seq_len)\n",
        "            selected_indices = top_indices[:, :, i]\n",
        "\n",
        "            # We will unsqueeze now because of multiplications later.\n",
        "            # (batch_size, seq_len, 1)\n",
        "            selected_weights = top_weights[:, :, i].unsqueeze(dim=-1)\n",
        "\n",
        "            # output is like the input\n",
        "            # (batch_size, seq_len, dim)\n",
        "            out = torch.zeros_like(x)\n",
        "\n",
        "            # Go through all the expert indices.\n",
        "            for expert_id in range(self.num_experts):\n",
        "\n",
        "                # Is this expert in the ith topmost indices selected above?\n",
        "                # (batch_size, seq_len)\n",
        "                mask = (expert_id == selected_indices)\n",
        "\n",
        "                if mask.any():\n",
        "\n",
        "                    # If there's at least one instance in the current batch\n",
        "                    # where this expert is in the ith topk slice, then evaluate.\n",
        "                    # (batch_size, seq_len, dim)\n",
        "                    expert_out = self.experts[expert_id](x)\n",
        "\n",
        "                    # Mask out those experts which are not present, and only highlight this one (expert_id).\n",
        "                    # Rest of the experts will be weighted 0.\n",
        "                    # (batch_size, seq_len, 1)\n",
        "                    masked_weights = selected_weights * mask.unsqueeze(dim=-1).float()\n",
        "\n",
        "                    # print(masked_weights.shape, expert_out.shape)\n",
        "\n",
        "                    # Now multiply the expert output with these weights and accumulate.\n",
        "                    # (batch_size, seq_len, dim)\n",
        "                    out += masked_weights * expert_out\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "2wgTzTem7K3y"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "moe = MoE(num_experts=4, dim=768, max_experts=2)"
      ],
      "metadata": {
        "id": "f_iCYQNgdU5O"
      },
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.randn(10, 64, 768)"
      ],
      "metadata": {
        "id": "A5-SUCwEijzE"
      },
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(moe(input).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p67EfDFb6itb",
        "outputId": "1427a63e-7139-4230-81d1-60fa11a60282"
      },
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 64, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1vS-xMk-6kCY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}