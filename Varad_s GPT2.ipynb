{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1dI1fkwm8ZUScUAP1sOv3lqk-1wMszIuD","authorship_tag":"ABX9TyORNGlI+wPZbF92wpnzBGl8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ab946b00716f4d6f888eaf3d5a89e6c3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0633ae7102f049a3beef0614d28b38ec","IPY_MODEL_37636fc731454c9c89aea1125764cc31","IPY_MODEL_2351e683780c44188dcd09d2865797eb"],"layout":"IPY_MODEL_5949828659ec42449edf2caed9fbae3e"}},"0633ae7102f049a3beef0614d28b38ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e96e51111e4f41afa11760252da67abc","placeholder":"​","style":"IPY_MODEL_5a7b4f2c3f134de5997594bff3027b61","value":"config.json: 100%"}},"37636fc731454c9c89aea1125764cc31":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3acafce86925408bb4124725fd5ad644","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_951c361ed1a5410a87969bd9a35b1223","value":665}},"2351e683780c44188dcd09d2865797eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25bde8e673184c52b55ab1bc8547917a","placeholder":"​","style":"IPY_MODEL_a40d18dacff34f2f91eacb9e8a8ad0d5","value":" 665/665 [00:00&lt;00:00, 81.8kB/s]"}},"5949828659ec42449edf2caed9fbae3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e96e51111e4f41afa11760252da67abc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a7b4f2c3f134de5997594bff3027b61":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3acafce86925408bb4124725fd5ad644":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"951c361ed1a5410a87969bd9a35b1223":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"25bde8e673184c52b55ab1bc8547917a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a40d18dacff34f2f91eacb9e8a8ad0d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3b50cf1499645f497b8c1d8177da91d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_136a5ee4df424fddb2e6da52281d15bc","IPY_MODEL_e8213ab8f9d54b1d88abccfb2fac7932","IPY_MODEL_d67d0a761c304cc082659e986739779d"],"layout":"IPY_MODEL_b1fff15f150441cba4ccd2bbeb2b7998"}},"136a5ee4df424fddb2e6da52281d15bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75fb320352514a18815cf587206079c5","placeholder":"​","style":"IPY_MODEL_3db6a1a5907847ebbf2e09630a65fe59","value":"model.safetensors: 100%"}},"e8213ab8f9d54b1d88abccfb2fac7932":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fea5a0221b4493eb2b777e3207d67f4","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_96374ef19e514183a902b0613e1579be","value":548105171}},"d67d0a761c304cc082659e986739779d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c724fd6087714b14b9f2bd899356d8c5","placeholder":"​","style":"IPY_MODEL_39dc1320c8204425888c32de2cc58c9c","value":" 548M/548M [00:03&lt;00:00, 198MB/s]"}},"b1fff15f150441cba4ccd2bbeb2b7998":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75fb320352514a18815cf587206079c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3db6a1a5907847ebbf2e09630a65fe59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fea5a0221b4493eb2b777e3207d67f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96374ef19e514183a902b0613e1579be":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c724fd6087714b14b9f2bd899356d8c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39dc1320c8204425888c32de2cc58c9c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fadf722e7bb434d8fe845c1810aa634":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2567781c56cc4606b5d1d8c5fd3dfbd5","IPY_MODEL_8584b9fee75f43bcb2e5c79413fecdf1","IPY_MODEL_fcc05545b3594fc099a9671b2a26d9e7"],"layout":"IPY_MODEL_2fef04e742e64e559fddec8ec34fed23"}},"2567781c56cc4606b5d1d8c5fd3dfbd5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad2d15f048674c4b8d38da50f53869e9","placeholder":"​","style":"IPY_MODEL_f904572c17aa4df18085d38866d468c1","value":"generation_config.json: 100%"}},"8584b9fee75f43bcb2e5c79413fecdf1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d00aedf8a1b46ca95f850356f735768","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7211c9ad9e8b416c83e23defc1dad9e4","value":124}},"fcc05545b3594fc099a9671b2a26d9e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c41c3a7e29c64861baa014a507096e9f","placeholder":"​","style":"IPY_MODEL_766efa108d21492d8a2143f6c51ef99c","value":" 124/124 [00:00&lt;00:00, 17.1kB/s]"}},"2fef04e742e64e559fddec8ec34fed23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad2d15f048674c4b8d38da50f53869e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f904572c17aa4df18085d38866d468c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d00aedf8a1b46ca95f850356f735768":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7211c9ad9e8b416c83e23defc1dad9e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c41c3a7e29c64861baa014a507096e9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"766efa108d21492d8a2143f6c51ef99c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kREU7DhPvNEj","executionInfo":{"status":"ok","timestamp":1742607445311,"user_tz":240,"elapsed":3516,"user":{"displayName":"Varad Deshmukh","userId":"07979260834662041934"}},"outputId":"c270f7f3-554f-4905-e43f-6df6cc9f54c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n","Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","Successfully installed tiktoken-0.9.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZKIfFzoBOPC"},"outputs":[],"source":["import os\n","import math\n","import time\n","import inspect\n","from dataclasses import dataclass\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from hellaswag import render_example, iterate_examples\n","import tiktoken\n","import random\n"]},{"cell_type":"code","source":["device = 'cuda:0'\n","device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\""],"metadata":{"id":"3RQ5JzHLvMim"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel\n"],"metadata":{"id":"fDGrqshRvCTB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = GPT2LMHeadModel.from_pretrained('gpt2')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237,"referenced_widgets":["ab946b00716f4d6f888eaf3d5a89e6c3","0633ae7102f049a3beef0614d28b38ec","37636fc731454c9c89aea1125764cc31","2351e683780c44188dcd09d2865797eb","5949828659ec42449edf2caed9fbae3e","e96e51111e4f41afa11760252da67abc","5a7b4f2c3f134de5997594bff3027b61","3acafce86925408bb4124725fd5ad644","951c361ed1a5410a87969bd9a35b1223","25bde8e673184c52b55ab1bc8547917a","a40d18dacff34f2f91eacb9e8a8ad0d5","a3b50cf1499645f497b8c1d8177da91d","136a5ee4df424fddb2e6da52281d15bc","e8213ab8f9d54b1d88abccfb2fac7932","d67d0a761c304cc082659e986739779d","b1fff15f150441cba4ccd2bbeb2b7998","75fb320352514a18815cf587206079c5","3db6a1a5907847ebbf2e09630a65fe59","6fea5a0221b4493eb2b777e3207d67f4","96374ef19e514183a902b0613e1579be","c724fd6087714b14b9f2bd899356d8c5","39dc1320c8204425888c32de2cc58c9c","6fadf722e7bb434d8fe845c1810aa634","2567781c56cc4606b5d1d8c5fd3dfbd5","8584b9fee75f43bcb2e5c79413fecdf1","fcc05545b3594fc099a9671b2a26d9e7","2fef04e742e64e559fddec8ec34fed23","ad2d15f048674c4b8d38da50f53869e9","f904572c17aa4df18085d38866d468c1","9d00aedf8a1b46ca95f850356f735768","7211c9ad9e8b416c83e23defc1dad9e4","c41c3a7e29c64861baa014a507096e9f","766efa108d21492d8a2143f6c51ef99c"]},"id":"0WCX3f_XwilG","executionInfo":{"status":"ok","timestamp":1742607467919,"user_tz":240,"elapsed":4022,"user":{"displayName":"Varad Deshmukh","userId":"07979260834662041934"}},"outputId":"5c6ae440-762d-4de3-c371-3eb3ca4178e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab946b00716f4d6f888eaf3d5a89e6c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3b50cf1499645f497b8c1d8177da91d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fadf722e7bb434d8fe845c1810aa634"}},"metadata":{}}]},{"cell_type":"code","source":["sd_hf = model.state_dict()\n","\n","for k, v in sd_hf.items():\n","  print(k, v.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VVrU0Ja_xR0k","executionInfo":{"status":"ok","timestamp":1742610369010,"user_tz":240,"elapsed":4,"user":{"displayName":"Varad Deshmukh","userId":"07979260834662041934"}},"outputId":"22361870-2ec8-4c99-8a4b-60e90a7083b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["transformer.wte.weight torch.Size([50257, 768])\n","transformer.wpe.weight torch.Size([1024, 768])\n","transformer.h.0.ln_1.weight torch.Size([768])\n","transformer.h.0.ln_1.bias torch.Size([768])\n","transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n","transformer.h.0.attn.c_attn.bias torch.Size([2304])\n","transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n","transformer.h.0.attn.c_proj.bias torch.Size([768])\n","transformer.h.0.ln_2.weight torch.Size([768])\n","transformer.h.0.ln_2.bias torch.Size([768])\n","transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n","transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n","transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n","transformer.h.0.mlp.c_proj.bias torch.Size([768])\n","transformer.h.1.ln_1.weight torch.Size([768])\n","transformer.h.1.ln_1.bias torch.Size([768])\n","transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n","transformer.h.1.attn.c_attn.bias torch.Size([2304])\n","transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n","transformer.h.1.attn.c_proj.bias torch.Size([768])\n","transformer.h.1.ln_2.weight torch.Size([768])\n","transformer.h.1.ln_2.bias torch.Size([768])\n","transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n","transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n","transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n","transformer.h.1.mlp.c_proj.bias torch.Size([768])\n","transformer.h.2.ln_1.weight torch.Size([768])\n","transformer.h.2.ln_1.bias torch.Size([768])\n","transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n","transformer.h.2.attn.c_attn.bias torch.Size([2304])\n","transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n","transformer.h.2.attn.c_proj.bias torch.Size([768])\n","transformer.h.2.ln_2.weight torch.Size([768])\n","transformer.h.2.ln_2.bias torch.Size([768])\n","transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n","transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n","transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n","transformer.h.2.mlp.c_proj.bias torch.Size([768])\n","transformer.h.3.ln_1.weight torch.Size([768])\n","transformer.h.3.ln_1.bias torch.Size([768])\n","transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n","transformer.h.3.attn.c_attn.bias torch.Size([2304])\n","transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n","transformer.h.3.attn.c_proj.bias torch.Size([768])\n","transformer.h.3.ln_2.weight torch.Size([768])\n","transformer.h.3.ln_2.bias torch.Size([768])\n","transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n","transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n","transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n","transformer.h.3.mlp.c_proj.bias torch.Size([768])\n","transformer.h.4.ln_1.weight torch.Size([768])\n","transformer.h.4.ln_1.bias torch.Size([768])\n","transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n","transformer.h.4.attn.c_attn.bias torch.Size([2304])\n","transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n","transformer.h.4.attn.c_proj.bias torch.Size([768])\n","transformer.h.4.ln_2.weight torch.Size([768])\n","transformer.h.4.ln_2.bias torch.Size([768])\n","transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n","transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n","transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n","transformer.h.4.mlp.c_proj.bias torch.Size([768])\n","transformer.h.5.ln_1.weight torch.Size([768])\n","transformer.h.5.ln_1.bias torch.Size([768])\n","transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n","transformer.h.5.attn.c_attn.bias torch.Size([2304])\n","transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n","transformer.h.5.attn.c_proj.bias torch.Size([768])\n","transformer.h.5.ln_2.weight torch.Size([768])\n","transformer.h.5.ln_2.bias torch.Size([768])\n","transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n","transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n","transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n","transformer.h.5.mlp.c_proj.bias torch.Size([768])\n","transformer.h.6.ln_1.weight torch.Size([768])\n","transformer.h.6.ln_1.bias torch.Size([768])\n","transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n","transformer.h.6.attn.c_attn.bias torch.Size([2304])\n","transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n","transformer.h.6.attn.c_proj.bias torch.Size([768])\n","transformer.h.6.ln_2.weight torch.Size([768])\n","transformer.h.6.ln_2.bias torch.Size([768])\n","transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n","transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n","transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n","transformer.h.6.mlp.c_proj.bias torch.Size([768])\n","transformer.h.7.ln_1.weight torch.Size([768])\n","transformer.h.7.ln_1.bias torch.Size([768])\n","transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n","transformer.h.7.attn.c_attn.bias torch.Size([2304])\n","transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n","transformer.h.7.attn.c_proj.bias torch.Size([768])\n","transformer.h.7.ln_2.weight torch.Size([768])\n","transformer.h.7.ln_2.bias torch.Size([768])\n","transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n","transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n","transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n","transformer.h.7.mlp.c_proj.bias torch.Size([768])\n","transformer.h.8.ln_1.weight torch.Size([768])\n","transformer.h.8.ln_1.bias torch.Size([768])\n","transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n","transformer.h.8.attn.c_attn.bias torch.Size([2304])\n","transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n","transformer.h.8.attn.c_proj.bias torch.Size([768])\n","transformer.h.8.ln_2.weight torch.Size([768])\n","transformer.h.8.ln_2.bias torch.Size([768])\n","transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n","transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n","transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n","transformer.h.8.mlp.c_proj.bias torch.Size([768])\n","transformer.h.9.ln_1.weight torch.Size([768])\n","transformer.h.9.ln_1.bias torch.Size([768])\n","transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n","transformer.h.9.attn.c_attn.bias torch.Size([2304])\n","transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n","transformer.h.9.attn.c_proj.bias torch.Size([768])\n","transformer.h.9.ln_2.weight torch.Size([768])\n","transformer.h.9.ln_2.bias torch.Size([768])\n","transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n","transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n","transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n","transformer.h.9.mlp.c_proj.bias torch.Size([768])\n","transformer.h.10.ln_1.weight torch.Size([768])\n","transformer.h.10.ln_1.bias torch.Size([768])\n","transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n","transformer.h.10.attn.c_attn.bias torch.Size([2304])\n","transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n","transformer.h.10.attn.c_proj.bias torch.Size([768])\n","transformer.h.10.ln_2.weight torch.Size([768])\n","transformer.h.10.ln_2.bias torch.Size([768])\n","transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n","transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n","transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n","transformer.h.10.mlp.c_proj.bias torch.Size([768])\n","transformer.h.11.ln_1.weight torch.Size([768])\n","transformer.h.11.ln_1.bias torch.Size([768])\n","transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n","transformer.h.11.attn.c_attn.bias torch.Size([2304])\n","transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n","transformer.h.11.attn.c_proj.bias torch.Size([768])\n","transformer.h.11.ln_2.weight torch.Size([768])\n","transformer.h.11.ln_2.bias torch.Size([768])\n","transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n","transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n","transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n","transformer.h.11.mlp.c_proj.bias torch.Size([768])\n","transformer.ln_f.weight torch.Size([768])\n","transformer.ln_f.bias torch.Size([768])\n","lm_head.weight torch.Size([50257, 768])\n"]}]},{"cell_type":"code","source":["@dataclass\n","class GPTConfig:\n","    block_size: int = 1024 # max sequence length\n","    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n","    n_layer: int = 12 # number of layers\n","    n_head: int = 12 # number of heads\n","    n_embd: int = 768 # embedding dimension"],"metadata":{"id":"pm-DBDZ61pV0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MLP(nn.Module):\n","\n","    def __init__(self, config):\n","      super().__init__()\n","      self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n","      self.gelu = nn.GELU(approximate='tanh')\n","      self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n","      self.c_proj.NANOGPT_SCALE_INIT = 1\n","\n","    def forward(self, x):\n","\n","      x = self.c_fc(x)\n","      x = self.gelu(x)\n","      x = self.c_proj(x)\n","\n","      return x\n","\n","class CausalSelfAttention(nn.Module):\n","\n","    def __init__(self, config):\n","      super().__init__()\n","\n","      self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n","      self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n","      self.c_proj.NANOGPT_SCALE_INIT = 1\n","\n","      self.n_head = config.n_head\n","      self.n_embd = config.n_embd\n","\n","    def forward(self, x):\n","\n","      B, T, C = x.size()\n","\n","      qkv = self.c_attn(x) # B, T, 3*C\n","      q, k, v = qkv.split(self.n_embd, dim=2) # each is of B, T, C\n","\n","      k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # B, nh, T, head_size\n","      q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # B, nh, T, head_size\n","      v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # B, nh, T, head_size\n","      y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # B, nh, T, head_size\n","      y = y.transpose(1,2).contiguous().view(B, T, C) # B, T, C\n","      y = self.c_proj(y) # B, T, C\n","\n","      return y\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, config):\n","      super().__init__()\n","      self.ln_1 = nn.LayerNorm(config.n_embd)\n","      self.attn = CausalSelfAttention(config)\n","      self.ln_2 = nn.LayerNorm(config.n_embd)\n","      self.mlp = MLP(config)\n","\n","\n","    def forward(self, x):\n","      x = x + self.attn(self.ln_1(x))\n","      x = x + self.mlp(self.ln_2(x))\n","      return x\n","\n","class GPT(nn.Module):\n","\n","    def __init__(self, config):\n","      super().__init__()\n","      self.config = config\n","\n","      self.transformer = nn.ModuleDict({\n","          'wte': nn.Embedding(config.vocab_size, config.n_embd),\n","          'wpe': nn.Embedding(config.block_size, config.n_embd),\n","          'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n","          'ln_f': nn.LayerNorm(config.n_embd),\n","      })\n","\n","      self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n","      self.transformer.wte.weight = self.lm_head.weight\n","\n","      self.apply(self._init_weights)\n","\n","      self.enc = tiktoken.get_encoding(\"gpt2\")\n","\n","\n","    def _init_weights(self, module):\n","      if isinstance(module, nn.Linear):\n","          std = 0.02\n","          if hasattr(module, 'NANOGPT_SCALE_INIT'):\n","              std *= (2 * self.config.n_layer) ** -0.5\n","          torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n","          if module.bias is not None:\n","              torch.nn.init.zeros_(module.bias)\n","      elif isinstance(module, nn.Embedding):\n","          torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, idx, targets=None):\n","\n","      B, T = idx.size()\n","\n","      pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n","\n","      tok_emb = self.transformer.wte(idx)\n","      pos_emb = self.transformer.wpe(pos)\n","      x = tok_emb + pos_emb\n","\n","      for block in self.transformer.h:\n","          x = block(x)\n","      x = self.transformer.ln_f(x)\n","\n","      logits = self.lm_head(x)\n","\n","      if targets == None:\n","          return logits, None\n","\n","      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n","\n","      return logits, loss\n","\n","    @classmethod\n","    def from_pretrained(cls, model_type):\n","\n","        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n","\n","        config_args = {\n","            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n","            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n","            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n","            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n","        }[model_type]\n","\n","        config_args['block_size'] = 1024\n","        config_args['vocab_size'] = 50257\n","\n","        config = GPTConfig(**config_args)\n","\n","        # Load my model, and get keys\n","\n","        model = GPT(config)\n","        sd = model.state_dict()\n","        sd_keys = sd.keys()\n","        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n","\n","        # Load the HF GPT 2 model transformer.\n","        hf_model = GPT2LMHeadModel.from_pretrained(model_type)\n","        sd_hf = hf_model.state_dict()\n","        sd_hf_keys = sd_hf.keys()\n","\n","        # We remove certain keys fromt the hf model.\n","        sd_hf_keys = [k for k in sd_hf_keys if not k.endswith('. c_attn.masked_bias')]\n","        sd_hf_keys = [k for k in sd_hf_keys if not k.endswith('.attn.bias')]\n","\n","        assert len(sd_hf_keys) == len(sd_keys), f\"mismatched keys {len(sd_hf_keys)} != {len(sd_keys)}\"\n","\n","        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n","\n","        for k in sd_hf_keys:\n","\n","            if any(k.endswith(w) for w in transposed):\n","                assert sd_hf[k].shape[::-1] == sd[k].shape\n","                with torch.no_grad():\n","                    sd[k].copy_(sd_hf[k].t())\n","            else:\n","                sd[k].copy_(sd_hf[k])\n","\n","        return model\n","\n","    def generate(self, x, max_length):\n","\n","        x = self.enc.encode(x)\n","\n","        x = torch.tensor(x).to(device).view(1,-1)\n","\n","        for iter in range(max_length):\n","\n","          logits, loss = self(x)\n","          out = logits[:, -1, :]\n","\n","          out_softmax = nn.Softmax(dim=1)(out)\n","          top_k_probs, top_k_idx = torch.topk(out_softmax, 50)\n","\n","          select = torch.multinomial(top_k_probs, 1)\n","          out = torch.gather(top_k_idx, -1, select)\n","          x = torch.concat((x, out), dim=-1)\n","\n","        out_list = x.cpu().detach().tolist()\n","\n","\n","        return enc.decode(out_list[0])\n","\n","    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n","\n","        param_dict = {pn: p for pn, p in self.named_parameters()}\n","        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n","\n","        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n","        non_decay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n","\n","        optim_groups = [\n","            {'params': decay_params, 'weight_decay': weight_decay},\n","            {'params': non_decay_params, 'weight_decay': 0.0}\n","        ]\n","\n","        num_decay_params = sum(p.numel() for p in decay_params)\n","        num_nodecay_params = sum(p.numel() for p in non_decay_params)\n","\n","        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n","        use_fused = fused_available and device_type == 'cuda'\n","\n","        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n","\n","        return optimizer\n"],"metadata":{"id":"cmrfV7wXy3Ng"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.set_float32_matmul_precision('high')\n","\n","# model = GPT.from_pretrained(\"gpt2\") # or init from OpenAI GPT-2\n","model = GPT(GPTConfig(vocab_size=50304))\n","model.to(device)\n","# model.eval()\n","model = torch.compile(model)"],"metadata":{"id":"-8BfW2JPG5xu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"Yy2oeVHQTXN7"}},{"cell_type":"code","source":["class DataLoader:\n","\n","    def __init__(self, B, T):\n","        super().__init__()\n","        self.B = B\n","        self.T = T\n","        self.enc = tiktoken.get_encoding(\"gpt2\")\n","\n","        torch.manual_seed(1337)\n","\n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed(1337)\n","        with open('input.txt', 'r') as f:\n","            lines = f.readlines()\n","\n","        self.current_position = 0\n","\n","        self.input_data = self.enc.encode(''.join(lines))\n","\n","    def get_next_batch(self):\n","\n","        B, T = self.B, self.T\n","\n","        idx = self.current_position\n","        # print(self.lines[idx: idx + B*T])\n","        inputs = torch.tensor(self.input_data[idx: idx + B*T]).view(B, T)\n","        targets = torch.tensor(self.input_data[idx + 1: idx + B*T + 1]).view(B, T)\n","\n","        self.current_position += B*T\n","\n","        if self.current_position > len(self.input_data) - B*T - 1:\n","            self.current_position = 0\n","\n","        return inputs, targets"],"metadata":{"id":"_wWNEzMdWVn5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_lr = 3e-4\n","min_lr = 3e-5\n","warm_up_steps = 10\n","max_steps = 50\n","\n","def get_lr(step):\n","\n","    if step < warm_up_steps:\n","        return (step + 1) / warm_up_steps * max_lr\n","\n","    if step > max_steps:\n","        return min_lr\n","\n","    decay_ratio = (step - warm_up_steps) / (max_steps - warm_up_steps)\n","    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n","\n","    return min_lr + coeff * (max_lr - min_lr)"],"metadata":{"id":"IFnG0Ksg8UkA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n","optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=max_lr, device_type=device_type)\n","\n","total_batch_size = 524288\n","B = 16\n","T = 1024\n","total_grad_accum = total_batch_size // (B * T)\n","loader = DataLoader(B, T)"],"metadata":{"id":"5MEL1ZQWcQYX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(total_grad_accum)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"35Z6bsUArUfX","executionInfo":{"status":"ok","timestamp":1742610454846,"user_tz":240,"elapsed":7,"user":{"displayName":"Varad Deshmukh","userId":"07979260834662041934"}},"outputId":"af9d0c4e-1b40-4b39-a33c-fcd1d5cc70ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["32\n"]}]},{"cell_type":"code","source":["for epoch in range(max_steps):\n","\n","    t0 = time.time()\n","\n","    lr = get_lr(epoch)\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","    optimizer.zero_grad()\n","\n","    total_loss_accum = 0.0\n","\n","    for micro_batch in range(total_grad_accum):\n","\n","      inputs, targets = loader.get_next_batch()\n","      inputs, targets = inputs.to(device), targets.to(device)\n","\n","      with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n","          logits, loss = model(inputs, targets)\n","          loss = loss / (total_grad_accum)\n","\n","      total_loss_accum += loss\n","\n","      loss.backward()\n","\n","    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","    if device_type == \"cuda\":\n","        torch.cuda.synchronize()\n","\n","    t1 = time.time()\n","    dt = t1 - t0\n","\n","    optimizer.step()\n","\n","    tokens_processed = loader.B * loader.T * total_grad_accum\n","    tokens_per_sec = tokens_processed / dt\n","\n","    if epoch % 1 == 0:\n","        print(f\"Epoch {epoch}, Train loss: {total_loss_accum:.3f}, Time: {dt:0.3f}, Norm: {norm:.3f}, Tokens: {tokens_processed}, Tokens/sec: {tokens_per_sec:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kuEXGO_jmNmv","executionInfo":{"status":"ok","timestamp":1742610848118,"user_tz":240,"elapsed":143989,"user":{"displayName":"Varad Deshmukh","userId":"07979260834662041934"}},"outputId":"49db49b6-0139-48ea-f964-d4f95296ce75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Train loss: 5.757, Time: 2.847, Norm: 0.374, Tokens: 524288, Tokens/sec: 184125.98\n","Epoch 1, Train loss: 5.741, Time: 2.844, Norm: 0.955, Tokens: 524288, Tokens/sec: 184358.43\n","Epoch 2, Train loss: 5.759, Time: 2.844, Norm: 0.796, Tokens: 524288, Tokens/sec: 184368.38\n","Epoch 3, Train loss: 5.713, Time: 2.852, Norm: 0.433, Tokens: 524288, Tokens/sec: 183859.95\n","Epoch 4, Train loss: 5.702, Time: 2.851, Norm: 0.363, Tokens: 524288, Tokens/sec: 183879.46\n","Epoch 5, Train loss: 5.703, Time: 2.857, Norm: 1.176, Tokens: 524288, Tokens/sec: 183521.71\n","Epoch 6, Train loss: 6.438, Time: 2.857, Norm: 55.000, Tokens: 524288, Tokens/sec: 183511.44\n","Epoch 7, Train loss: 5.799, Time: 2.861, Norm: 2.936, Tokens: 524288, Tokens/sec: 183222.53\n","Epoch 8, Train loss: 5.718, Time: 2.859, Norm: 1.644, Tokens: 524288, Tokens/sec: 183360.27\n","Epoch 9, Train loss: 5.747, Time: 2.864, Norm: 3.337, Tokens: 524288, Tokens/sec: 183076.00\n","Epoch 10, Train loss: 5.669, Time: 2.865, Norm: 0.869, Tokens: 524288, Tokens/sec: 183010.81\n","Epoch 11, Train loss: 5.684, Time: 2.869, Norm: 1.743, Tokens: 524288, Tokens/sec: 182742.15\n","Epoch 12, Train loss: 5.646, Time: 2.866, Norm: 1.748, Tokens: 524288, Tokens/sec: 182912.70\n","Epoch 13, Train loss: 5.620, Time: 2.871, Norm: 3.284, Tokens: 524288, Tokens/sec: 182621.42\n","Epoch 14, Train loss: 5.609, Time: 2.873, Norm: 1.645, Tokens: 524288, Tokens/sec: 182499.42\n","Epoch 15, Train loss: 5.614, Time: 2.874, Norm: 2.283, Tokens: 524288, Tokens/sec: 182453.25\n","Epoch 16, Train loss: 5.542, Time: 2.872, Norm: 1.237, Tokens: 524288, Tokens/sec: 182532.20\n","Epoch 17, Train loss: 5.626, Time: 2.875, Norm: 4.305, Tokens: 524288, Tokens/sec: 182338.74\n","Epoch 18, Train loss: 5.592, Time: 2.877, Norm: 4.010, Tokens: 524288, Tokens/sec: 182220.75\n","Epoch 19, Train loss: 5.511, Time: 2.878, Norm: 1.108, Tokens: 524288, Tokens/sec: 182180.65\n","Epoch 20, Train loss: 5.533, Time: 2.875, Norm: 2.594, Tokens: 524288, Tokens/sec: 182360.94\n","Epoch 21, Train loss: 5.530, Time: 2.880, Norm: 2.966, Tokens: 524288, Tokens/sec: 182030.94\n","Epoch 22, Train loss: 5.496, Time: 2.881, Norm: 1.304, Tokens: 524288, Tokens/sec: 182010.90\n","Epoch 23, Train loss: 5.444, Time: 2.879, Norm: 1.807, Tokens: 524288, Tokens/sec: 182088.20\n","Epoch 24, Train loss: 5.452, Time: 2.879, Norm: 2.211, Tokens: 524288, Tokens/sec: 182121.89\n","Epoch 25, Train loss: 5.419, Time: 2.882, Norm: 1.473, Tokens: 524288, Tokens/sec: 181923.25\n","Epoch 26, Train loss: 5.368, Time: 2.884, Norm: 1.019, Tokens: 524288, Tokens/sec: 181792.04\n","Epoch 27, Train loss: 5.388, Time: 2.884, Norm: 1.132, Tokens: 524288, Tokens/sec: 181773.78\n","Epoch 28, Train loss: 5.343, Time: 2.881, Norm: 1.272, Tokens: 524288, Tokens/sec: 182008.77\n","Epoch 29, Train loss: 5.322, Time: 2.882, Norm: 0.894, Tokens: 524288, Tokens/sec: 181931.18\n","Epoch 30, Train loss: 5.302, Time: 2.884, Norm: 0.707, Tokens: 524288, Tokens/sec: 181762.58\n","Epoch 31, Train loss: 5.278, Time: 2.884, Norm: 0.940, Tokens: 524288, Tokens/sec: 181821.97\n","Epoch 32, Train loss: 5.289, Time: 2.884, Norm: 0.763, Tokens: 524288, Tokens/sec: 181795.51\n","Epoch 33, Train loss: 5.226, Time: 2.884, Norm: 0.610, Tokens: 524288, Tokens/sec: 181805.90\n","Epoch 34, Train loss: 5.224, Time: 2.888, Norm: 0.641, Tokens: 524288, Tokens/sec: 181567.72\n","Epoch 35, Train loss: 5.213, Time: 2.886, Norm: 0.670, Tokens: 524288, Tokens/sec: 181655.16\n","Epoch 36, Train loss: 5.185, Time: 2.882, Norm: 0.644, Tokens: 524288, Tokens/sec: 181888.85\n","Epoch 37, Train loss: 5.197, Time: 2.885, Norm: 0.534, Tokens: 524288, Tokens/sec: 181746.94\n","Epoch 38, Train loss: 5.139, Time: 2.886, Norm: 0.384, Tokens: 524288, Tokens/sec: 181655.77\n","Epoch 39, Train loss: 5.150, Time: 2.891, Norm: 0.491, Tokens: 524288, Tokens/sec: 181340.58\n","Epoch 40, Train loss: 5.138, Time: 2.886, Norm: 0.516, Tokens: 524288, Tokens/sec: 181675.06\n","Epoch 41, Train loss: 5.109, Time: 2.890, Norm: 0.410, Tokens: 524288, Tokens/sec: 181392.53\n","Epoch 42, Train loss: 5.128, Time: 2.888, Norm: 0.299, Tokens: 524288, Tokens/sec: 181542.44\n","Epoch 43, Train loss: 5.075, Time: 2.890, Norm: 0.358, Tokens: 524288, Tokens/sec: 181435.75\n","Epoch 44, Train loss: 5.090, Time: 2.887, Norm: 0.420, Tokens: 524288, Tokens/sec: 181577.51\n","Epoch 45, Train loss: 5.080, Time: 2.886, Norm: 0.399, Tokens: 524288, Tokens/sec: 181653.79\n","Epoch 46, Train loss: 5.057, Time: 2.890, Norm: 0.349, Tokens: 524288, Tokens/sec: 181406.33\n","Epoch 47, Train loss: 5.081, Time: 2.895, Norm: 0.303, Tokens: 524288, Tokens/sec: 181126.09\n","Epoch 48, Train loss: 5.029, Time: 2.888, Norm: 0.340, Tokens: 524288, Tokens/sec: 181514.62\n","Epoch 49, Train loss: 5.046, Time: 2.887, Norm: 0.315, Tokens: 524288, Tokens/sec: 181592.32\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"8nkw2sBBmNDx"}},{"cell_type":"code","source":["\n"],"metadata":{"id":"yxvgsMWwd25g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z8JB2RtMhJp7","executionInfo":{"status":"ok","timestamp":1741383218492,"user_tz":300,"elapsed":5,"user":{"displayName":"Varad Deshmukh","userId":"07979260834662041934"}},"outputId":"5a37fa27-0aa6-42d6-958c-c2b5f10e1491"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(10.9384, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"J_Cxj-thhcbA"},"execution_count":null,"outputs":[]}]}